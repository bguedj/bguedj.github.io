@mastersthesis{guedj2010,
author={Guedj, Benjamin},
title = {A {Bayesian} modelling of the hybridization mechanism},
year = {2010},
school = {Sorbonne Université and Danmarks Tekniske Universitet},
keywords={mine}
}

@article{guedj2011,
author = {Guedj, Benjamin and Guillot, Gilles},
title = {Estimating the location and shape of hybrid zones},
journal = {Molecular Ecology Resources},
volume = {11},
number = {6},
pages = {1119-1123},
year = {2011},
abstract="We propose a new model to make use of georeferenced genetic data for inferring the location and shape of a hybrid zone. The model output includes the posterior distribution of a parameter that quantifies the width of the hybrid zone. The model proposed is implemented in the GUI and command‐line versions of the Geneland program versions $\geq$3.3.0. Information about the program can be found on http://www2.imm.dtu.dk/gigu/Geneland/.",
keywords = {Population structure, hybridization, admixture, selection pressure, Markov chain Monte Carlo, Geneland, mine},
doi = {10.1111/j.1755-0998.2011.03045.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1755-0998.2011.03045.x},
url_Code = {https://i-pri.org/special/Biostatistics/Software/Geneland/}
}

@article{guedj2013,
author = "Guedj, Benjamin and Alquier, Pierre",
doi = "10.1214/13-EJS771",
fjournal = "Electronic Journal of Statistics",
journal = "Electron. J. Statist.",
pages = "264--291",
abstract = "The present paper is about estimation and prediction in high-dimensional additive models under a sparsity assumption ($p\gg n$ paradigm). A PAC-Bayesian strategy is investigated, delivering oracle inequalities in probability. The implementation is performed through recent outcomes in high-dimensional MCMC algorithms, and the performance of our method is assessed on simulated data.",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = {{PAC-Bayesian} estimation and prediction in sparse additive models},
url = "https://doi.org/10.1214/13-EJS771",
volume = "7",
year = "2013",
url_arXiv = "https://arxiv.org/abs/1208.1211",
url_PDF = "https://arxiv.org/pdf/1208.1211.pdf",
url_Code = "https://cran.r-project.org/package=pacbpred",
keywords={mine}
}

@phdthesis{guedj2013phd,
TITLE = {Aggregation of estimators and classifiers: theory and methods},
AUTHOR = {Guedj, Benjamin},
URL = {https://tel.archives-ouvertes.fr/tel-00922353},
SCHOOL = {{Universit{\'e} Pierre et Marie Curie - Paris VI}},
YEAR = {2013},
MONTH = Dec,
abstract = "This thesis is devoted to the study of both theoretical and practical properties of various aggregation techniques. We first extend the PAC-Bayesian theory to the high dimensional paradigm in the additive and logistic regression settings. We prove that our estimators are nearly minimax optimal, and we provide an MCMC implementation, backed up by numerical simulations. Next, we introduce an original nonlinear aggregation strategy. Its theoretical merits are presented, and we benchmark the method—called COBRA—on a lengthy series of numerical experiments. Finally, a Bayesian approach to model admixture in population genetics is presented, along with its MCMC implementation. All approaches introduced in this thesis are freely available on the author’s website.",
KEYWORDS = {Aggregation ; oracle inequalities ; PAC-Bayesian theory ; sparsity ; Agr{\'e}gation ; r{\'e}gression ; classification ; in{\'e}galit{\'e}s oracles ; th{\'e}orie PAC-bay{\'e}sienne ; COBRA ; MCMC ; parcimonie, mine},
TYPE = {Theses},
url_PDF = "https://bguedj.github.io/files/bguedj-phd.pdf"
}

@article{chopin2015survey,
title={On some recent advances on high dimensional {Bayesian} statistics},
author={Chopin, Nicolas and Gadat, S{\'e}bastien and Guedj, Benjamin and Guyader, Arnaud and Vernet, Elodie},
journal={ESAIM: Proceedings and Surveys},
volume={51},
pages={293--319},
year={2015},
abstract = "This paper proposes to review some recent developments in Bayesian statistics for high dimensional data. After giving some brief motivations in a short introduction, we describe new advances in the understanding of Bayes posterior computation as well as theoretical contributions in non parametric and high dimensional Bayesian approaches. From an applied point of view, we describe the so-called SQMC particle method to compute posterior Bayesian law, and provide a nonparametric analysis of the widespread ABC method. On the theoretical side, we describe some recent advances in Bayesian consistency for a nonparametric hidden Markov model as well as new PAC-Bayesian results for different models of high dimensional regression.",
publisher={EDP Sciences},
DOI= "10.1051/proc/201551016",
url= "https://doi.org/10.1051/proc/201551016",
url_PDF="https://www.esaim-proc.org/articles/proc/pdf/2015/04/proc145116.pdf",
keywords={mine}
}

@article{biau2016cobra,
title={{COBRA}: A combined regression strategy},
author={Biau, G{\'e}rard and Fischer, Aur{\'e}lie and Guedj, Benjamin and Malley, James D.},
journal={Journal of Multivariate Analysis},
volume={146},
pages={18--28},
year={2016},
abstract = "A new method for combining several initial estimators of the regression function is introduced. Instead of building a linear or convex optimized combination over a collection of basic estimators $r_1,\dots,r_M$, we use them as a collective indicator of the proximity between the training data and a test observation. This local distance approach is model-free and very fast. More specifically, the resulting nonparametric/nonlinear combined estimator is shown to perform asymptotically at least as well in the $L^2$
sense as the best combination of the basic estimators in the collective. A companion R package called COBRA (standing for COmBined Regression Alternative) is presented (downloadable on http://cran.r-project.org/web/packages/COBRA/index.html). Substantial numerical evidence is provided on both synthetic and real data sets to assess the excellent performance and velocity of our method in a large variety of prediction problems.",
publisher={Elsevier},
note = "Special Issue on Statistical Models and Methods for High or Infinite Dimensional Spaces",
issn = "0047-259X",
doi = "https://doi.org/10.1016/j.jmva.2015.04.007",
url = "http://www.sciencedirect.com/science/article/pii/S0047259X15000950",
url_arxiv = "https://arxiv.org/abs/1303.2236",
url_PDF = "https://arxiv.org/pdf/1303.2236.pdf",
url_Code = "https://cran.r-project.org/package=COBRA",
keywords={mine}
}

@unpublished{celisse2016stability,
title={Stability revisited: new generalisation bounds for the {Leave-one-Out}},
author={Celisse, Alain and Guedj, Benjamin},
note={Preprint.},
url = "https://arxiv.org/abs/1608.06412",
url_PDF = "https://arxiv.org/pdf/1608.06412.pdf",
year={2016},
abstract = "The present paper provides a new generic strategy leading to non-asymptotic theoretical guarantees on the Leave-one-Out procedure applied to a broad class of learning algorithms. This strategy relies on two main ingredients: the new notion of $L^q$ stability, and the strong use of moment inequalities. $L^q$ stability extends the ongoing notion of hypothesis stability while remaining weaker than the uniform stability. It leads to new PAC exponential generalisation bounds for Leave-one-Out under mild assumptions. In the literature, such bounds are available only for uniform stable algorithms under boundedness for instance. Our generic strategy is applied to the Ridge regression algorithm as a first step.",
keywords={mine}
}

@article{alquier2017oracle,
title={An oracle inequality for quasi-{Bayesian} nonnegative matrix factorization},
author={Alquier, Pierre and Guedj, Benjamin},
journal={Mathematical Methods of Statistics},
volume={26},
number={1},
pages={55--67},
year={2017},
publisher={Springer},
abstract = "The aim of this paper is to provide some theoretical understanding of quasi-Bayesian aggregation methods non-negative matrix factorization. We derive an oracle inequality for an aggregated estimator. This result holds for a very general class of prior distributions and shows how the prior affects the rate of convergence.",
issn="1934-8045",
doi="10.3103/S1066530717010045",
url = "https://link.springer.com/article/10.3103%2FS1066530717010045",
url_arXiv="https://arxiv.org/abs/1601.01345",
url_PDF = "https://arxiv.org/pdf/1601.01345.pdf",
url_Code = "https://github.com/astha736/PACbayesianNMF",
keywords={mine}
}

@article{guedj2018pac,
title={{PAC-Bayesian} high dimensional bipartite ranking},
author={Guedj, Benjamin and Robbiano, Sylvain},
journal={Journal of Statistical Planning and Inference},
year={2018},
publisher={Elsevier},
volume = "196",
pages = "70 - 86",
abstract = "This paper is devoted to the bipartite ranking problem, a classical statistical learning task, in a high dimensional setting. We propose a scoring and ranking strategy based on the PAC-Bayesian approach. We consider nonlinear additive scoring functions, and we derive non-asymptotic risk bounds under a sparsity assumption. In particular, oracle inequalities in probability holding under a margin condition assess the performance of our procedure, and prove its minimax optimality. An MCMC-flavored algorithm is proposed to implement our method, along with its behavior on synthetic and real-life datasets.",
issn = "0378-3758",
doi = "https://doi.org/10.1016/j.jspi.2017.10.010",
url = "http://www.sciencedirect.com/science/article/pii/S0378375817301945",
url_arXiv = "https://arxiv.org/abs/1511.02729",
url_PDF = "https://arxiv.org/pdf/1511.02729.pdf",
keywords={mine}
}

@article{alquier2018simpler,
title={Simpler {PAC-Bayesian} bounds for hostile data},
author={Alquier, Pierre and Guedj, Benjamin},
journal={Machine Learning},
volume={107},
number={5},
pages={887--902},
year={2018},
publisher={Springer},
abstract = "PAC-Bayesian learning bounds are of the utmost interest to the learning community. Their role is to connect the generalization ability of an aggregation distribution $\rho$ to its empirical risk and to its Kullback-Leibler divergence with respect to some prior distribution $\pi$. Unfortunately, most of the available bounds typically rely on heavy assumptions such as boundedness and independence of the observations. This paper aims at relaxing these constraints and provides PAC-Bayesian learning bounds that hold for dependent, heavy-tailed observations (hereafter referred to as \emph{hostile data}). In these bounds the Kullack-Leibler divergence is replaced with a general version of Csiszár’s f-divergence. We prove a general PAC-Bayesian bound, and show how to use it in various hostile settings.",
issn="1573-0565",
doi="10.1007/s10994-017-5690-0",
url="https://doi.org/10.1007/s10994-017-5690-0",
url_arXiv = "https://arxiv.org/abs/1610.07193",
url_PDF = "https://arxiv.org/pdf/1610.07193.pdf",
keywords={mine}
}

@article{guedj2018pycobra,
author  = {Guedj, Benjamin and Srinivasa Desikan, Bhargav},
title   = {Pycobra: A {Python} Toolbox for Ensemble Learning and Visualisation},
journal = {Journal of Machine Learning Research},
year    = {2018},
volume  = {18},
number  = {190},
pages   = {1-5},
abstract = "We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations. pycobra is fully scikit-learn compatible and is released under the MIT open-source license. pycobra can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at https://github.com/bhargavvader/pycobra and official documentation website is https://modal.lille.inria.fr/pycobra.",
url     = {http://jmlr.org/beta/papers/v18/17-228.html},
url_arXiv = {https://arxiv.org/abs/1707.00558},
url_PDF = "http://jmlr.org/papers/volume18/17-228/17-228.pdf",
url_Code = "https://github.com/bhargavvader/pycobra",
keywords={mine}
}

@article{li2018,
author = "Li, Le and Guedj, Benjamin and Loustau, Sébastien",
doi = "10.1214/18-EJS1479",
fjournal = "Electronic Journal of Statistics",
journal = "Electron. J. Statist.",
number = "2",
pages = "3071--3113",
abstract = "When faced with high frequency streams of data, clustering raises theoretical and algorithmic pitfalls. We introduce a new and adaptive online clustering algorithm relying on a quasi-Bayesian approach, with a dynamic (i.e., time-dependent) estimation of the (unknown and changing) number of clusters. We prove that our approach is supported by minimax regret bounds. We also provide an RJMCMC-flavored implementation (called PACBO, see https://cran.r-project.org/web/packages/PACBO/index.html) for which we give a convergence guarantee. Finally, numerical experiments illustrate the potential of our procedure.",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = {A quasi-{Bayesian} perspective to online clustering},
url = "https://doi.org/10.1214/18-EJS1479",
url_arXiv = "https://arxiv.org/abs/1602.00522",
url_PDF = "https://arxiv.org/pdf/1602.00522.pdf",
url_Code = "https://cran.r-project.org/src/contrib/Archive/PACBO/",
volume = "12",
year = "2018",
keywords={mine}
}

@article{guedj2018sequential,
title={Sequential Learning of Principal Curves: Summarizing Data Streams on the Fly},
author={Li, Le and Guedj, Benjamin},
journal={Entropy},
volume = {23},
number = {11},
ARTICLE-NUMBER = {1534},
ISSN = {1099-4300},
doi = {10.3390/e23111534},
issuetitle = {Approximate Bayesian Inference},
url = {https://www.mdpi.com/1099-4300/23/11/1534},
url_arXiv = "https://arxiv.org/abs/1805.07418",
url_PDF = "https://www.mdpi.com/1099-4300/23/11/1534/pdf",
abstract = {When confronted with massive data streams, summarizing data with dimension reduction methods such as PCA raises theoretical and algorithmic pitfalls. A principal curve acts as a nonlinear generalization of PCA, and the present paper proposes a novel algorithm to automatically and sequentially learn principal curves from data streams. We show that our procedure is supported by regret bounds with optimal sublinear remainder terms. A greedy local search implementation (called slpc, for sequential learning principal curves) that incorporates both sleeping experts and multi-armed bandit ingredients is presented, along with its regret computation and performance on synthetic and real-life data.},
year={2021},
eprint={1805.07418},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@inproceedings{klein2018,
author={Klein, John and Albardan, Mahmoud and Guedj, Benjamin and Colot, Olivier},
editor="Cellier, Peggy
and Driessens, Kurt",
title={Decentralized Learning with Budgeted Network Load Using {Gaussian} Copulas and Classifier Ensembles},
booktitle="ECML-PKDD 2019: Machine Learning and Knowledge Discovery in Databases",
year="2020",
publisher="Springer International Publishing",
pages="301--316",
abstract="We examine a network of learners which address the same classification task but must learn from different data sets. The learners cannot share data but instead share their models. Models are shared only one time so as to preserve the network load. We introduce DELCO (standing for Decentralized Ensemble Learning with COpulas), a new approach allowing to aggregate the predictions of the classifiers trained by each learner. The proposed method aggregates the base classifiers using a probabilistic model relying on Gaussian copulas. Experiments on logistic regressor ensembles demonstrate competing accuracy and increased robustness in case of dependent classifiers. A companion python implementation can be downloaded at https://github.com/john-klein/DELCO.",
isbn="978-3-030-43823-4",
doi = "10.1007/978-3-030-43823-4_26",
url = "https://link.springer.com/chapter/10.1007%2F978-3-030-43823-4_26",
url_arXiv = "https://arxiv.org/abs/1804.10028",
url_PDF = "https://arxiv.org/pdf/1804.10028.pdf",
url_Code = "https://github.com/john-klein/DELCO",
keywords={mine}
}


@inproceedings{guedj2019primer,
title={A Primer on {PAC-Bayesian} Learning},
author={Guedj, Benjamin},
booktitle={Proceedings of the second congress of the French Mathematical Society},
year={2019},
volume = {33},
abstract = "Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.",
url = "https://arxiv.org/abs/1901.05353",
url_arXiv = "https://arxiv.org/abs/1901.05353",
url_PDF = "https://arxiv.org/pdf/1901.05353.pdf",
url_Video = "https://youtu.be/6t_vojO8jLI",
keywords={mine}
}

@inproceedings{chretien2019revisiting,
title={Revisiting clustering as matrix factorisation on the {Stiefel} manifold},
author={Chr{\'e}tien, St{\'e}phane and Guedj, Benjamin},
year={2020},
editor="Nicosia, Giuseppe
and Ojha, Varun
and La Malfa, Emanuele
and Jansen, Giorgio
and Sciacca, Vincenzo
and Pardalos, Panos
and Giuffrida, Giovanni
and Umeton, Renato",
publisher="Springer International Publishing",
pages="1--12",
abstract = "This paper studies clustering for possibly high dimensional data (e.g. images, time series, gene expression data, and many other settings), and rephrase it as low rank matrix estimation in the PAC-Bayesian framework. Our approach leverages the well known Burer-Monteiro factorisation strategy from large scale optimisation, in the context of low rank estimation. Moreover, our Burer-Monteiro factors are shown to lie on a Stiefel manifold. We propose a new generalized Bayesian estimator for this problem and prove novel prediction bounds for clustering. We also devise a componentwise Langevin sampler on the Stiefel manifold to compute this estimator.",
booktitle="LOD -- The Sixth International Conference on Machine Learning, Optimization, and Data Science",
isbn="978-3-030-64583-0",
url = "https://link.springer.com/chapter/10.1007%2F978-3-030-64583-0_1",
url_arXiv = "https://arxiv.org/abs/1903.04479",
url_PDF = "https://arxiv.org/pdf/1903.04479.pdf",
url_Video = "https://youtu.be/Gz2euWW8kyA",
DOI = "10.1007/978-3-030-64583-0_1",
keywords={mine}
}

@inproceedings{guedj2019nonlinear,
title={Non-linear aggregation of filters to improve image denoising},
author={Guedj, Benjamin and Rengot, Juliette},
booktitle="SAI: Intelligent Computing",
editor="Arai, Kohei
and Kapoor, Supriya
and Bhatia, Rahul",
publisher="Springer International Publishing",
pages="314--327",
abstract="We introduce a novel aggregation method to efficiently perform image denoising. Preliminary filters are aggregated in a non-linear fashion, using a new metric of pixel proximity based on how the pool of filters reaches a consensus. We provide a theoretical bound to support our aggregation scheme, its numerical performance is illustrated and we show that the aggregate significantly outperforms each of the preliminary filters.",
isbn="978-3-030-52246-9",
doi = "10.1007/978-3-030-52246-9_22",
year={2020},
eprint={1904.00865},
archivePrefix={arXiv},
url = "https://link.springer.com/chapter/10.1007%2F978-3-030-52246-9_22",
url_arXiv = "https://arxiv.org/abs/1904.00865",
url_PDF = "https://arxiv.org/pdf/1904.00865.pdf",
url_Code = "https://github.com/rengotj/cobra_denoising",
url_Video = "https://youtu.be/cX1DErrxzOk",
keywords={mine}
}

@article{zhang2019perturbation,
title={Model Validation Using Mutated Training Labels: An Exploratory Study},
author={Jie M. Zhang and Mark Harman and Benjamin Guedj and Earl T. Barr and John Shawe-Taylor},
year={2023},
note = "To appear",
journal = {Neurocomputing},
volume = {},
doi = {},
abstract = {We introduce an exploratory study on Mutation Validation (MV), a model
validation method using mutated training labels for supervised learning. MV
mutates training data labels, retrains the model against the mutated data, then
uses the metamorphic relation that captures the consequent training performance
changes to assess model fit. It does not use a validation set or test set. The
intuition underpinning MV is that overfitting models tend to fit noise in the
training data. We explore 8 different learning algorithms, 18 datasets, and 5
types of hyperparameter tuning tasks. Our results demonstrate that MV is
accurate in model selection: the model recommendation hit rate is 92\% for MV
and less than 60\% for out-of-sample-validation. MV also provides more stable
hyperparameter tuning results than out-of-sample-validation across different
runs.},
url = "https://arxiv.org/abs/1905.10201",
url_PDF = "https://arxiv.org/pdf/1905.10201.pdf",
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{letarte19dichotomize,
author    = {Ga{\"{e}}l Letarte and
Pascal Germain and
Benjamin Guedj and
Fran{\c{c}}ois Laviolette},
editor    = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
title     = {Dichotomize and Generalize: {PAC-Bayesian} Binary Activated Deep Neural
Networks},
booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems [NeurIPS] 2019, 8-14
December 2019, Vancouver, BC, Canada},
pages     = {6869--6879},
year      = {2019},
eprint = "https://arxiv.org/abs/1905.10259",
timestamp = {Fri, 06 Mar 2020 16:59:22 +0100},
biburl    = {https://dblp.org/rec/conf/nips/LetarteGGL19.bib},
bibsource = {dblp computer science bibliography, https://dblp.org},
abstract = "We present a comprehensive study of multilayer neural networks with binary activation, relying on the PAC-Bayesian theory. Our contributions are twofold: (i) we develop an end-to-end framework to train a binary activated deep neural network, (ii) we provide nonvacuous PAC-Bayesian generalization bounds for binary activated deep neural networks. Our results are obtained by minimizing the expected loss of an architecture-dependent aggregation of binary activated deep neural networks. Our analysis inherently overcomes the fact that binary activation function is non-differentiable. The performance of our approach is assessed on a thorough numerical experiment protocol on real-life datasets.",
url = "https://papers.nips.cc/paper/8911-dichotomize-and-generalize-pac-bayesian-binary-activated-deep-neural-networks",
url_arXiv = "https://arxiv.org/abs/1905.10259",
url_PDF = "https://papers.nips.cc/paper/8911-dichotomize-and-generalize-pac-bayesian-binary-activated-deep-neural-networks.pdf",
url_Code = "https://github.com/gletarte/dichotomize-and-generalize",
url_Video = "https://youtu.be/FcIE0AVrTDg",
keywords={mine}
}

@article{Alliez_2020,
title={Attributing and Referencing (Research) Software: Best Practices and Outlook From {Inria}},
volume={22},
ISSN={1558-366X},
url={http://dx.doi.org/10.1109/MCSE.2019.2949413},
url_arXiv = "https://arxiv.org/abs/1905.11123",
url_PDF = "https://arxiv.org/pdf/1905.11123.pdf",
DOI={10.1109/mcse.2019.2949413},
number={1},
abstract = "Software is a fundamental pillar of modern scientific research, across all fields and disciplines. However, there is a lack of adequate means to cite and reference software due to the complexity of the problem in terms of authorship, roles, and credits. This complexity is further increased when it is considered over the lifetime of a software that can span up to several decades. Building upon the internal experience of Inria, the French research institute for digital sciences, we provide in this article a contribution to the ongoing efforts in order to develop proper guidelines and recommendations for software citation and reference. Namely, we recommend: first, a richer taxonomy for software contributions with a qualitative scale; second, to put humans at the heart of the evaluation; and third, to distinguish citation from reference.",
journal={Computing in Science \& Engineering},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Alliez, Pierre and Di Cosmo, Roberto and Guedj, Benjamin and Girault, Alain and Hacid, Mohand-Said and Legrand, Arnaud and Rougier, Nicolas},
year={2020},
month={Jan},
pages={39–52},
keywords={mine}
}

@inproceedings{mhammedi19pac,
author    = {Zakaria Mhammedi and
Peter Gr{\"{u}}nwald and
Benjamin Guedj},
editor    = {Hanna M. Wallach and
Hugo Larochelle and
Alina Beygelzimer and
Florence d'Alch{\'{e}}{-}Buc and
Emily B. Fox and
Roman Garnett},
title     = {{PAC-Bayes} Un-Expected {Bernstein} Inequality},
booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems [NeurIPS] 2019, 8-14
December 2019, Vancouver, BC, Canada},
pages     = {12180--12191},
year      = {2019},
url       = {http://papers.nips.cc/paper/9387-pac-bayes-un-expected-bernstein-inequality},
timestamp = {Fri, 06 Mar 2020 16:59:25 +0100},
biburl    = {https://dblp.org/rec/conf/nips/MhammediGG19.bib},
abstract = "We present a new PAC-Bayesian generalization bound. Standard bounds contain a $\sqrt{L_n \cdot \mathrm{KL}/n}$ complexity term which dominates unless $L_n$, the empirical error of the learning algorithm's randomized predictions, vanishes. We manage to replace $L_n$ by a term which vanishes in many more situations, essentially whenever the employed learning algorithm is sufficiently stable on the dataset at hand. Our new bound consistently beats state-of-the-art bounds both on a toy example and on UCI datasets (with large enough $n$). Theoretically, unlike existing bounds, our new bound can be expected to converge to $0$ faster whenever a Bernstein/Tsybakov condition holds, thus connecting PAC-Bayesian generalization and {\em excess risk\/} bounds---for the latter it has long been known that faster convergence can be obtained under Bernstein conditions. Our main technical tool is a new concentration inequality which is like Bernstein's but with $X^2$ taken outside its expectation.",
bibsource = {dblp computer science bibliography, https://dblp.org},
url = "https://papers.nips.cc/paper/9387-pac-bayes-un-expected-bernstein-inequality",
url_arXiv = "https://arxiv.org/abs/1905.13367",
url_PDF = "https://papers.nips.cc/paper/9387-pac-bayes-un-expected-bernstein-inequality.pdf",
url_Code = "https://github.com/bguedj/PAC-Bayesian-Un-Expected-Bernstein-Inequality",
keywords={mine}
}


@InProceedings{pmlr-v130-cohen-addad21a,
title = 	 {Online k-means Clustering},
author =       {Cohen-Addad, Vincent and Guedj, Benjamin and Kanade, Varun and Rom, Guy},
booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics [AISTATS]},
pages = 	 {1126--1134},
year = 	 {2021},
editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
volume = 	 {130},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {April},
publisher =    {PMLR},
url = 	 {http://proceedings.mlr.press/v130/cohen-addad21a.html},
url_arXiv = "https://arxiv.org/abs/1909.06861",
url_PDF = 	 {http://proceedings.mlr.press/v130/cohen-addad21a/cohen-addad21a.pdf},
url_Supplementary = {http://proceedings.mlr.press/v130/cohen-addad21a/cohen-addad21a-supp.pdf},
url_Video = {https://slideslive.com/38953036},
abstract = 	 {We study the problem of learning a clustering of an online set of points. The specific formulation we use is the k-means objective: At each time step the algorithm has to maintain a set of k candidate centers and the loss incurred by the algorithm is the squared distance between the new point and the closest center. The goal is to minimize regret with respect to the best solution to the k-means objective in hindsight. We show that provided the data lies in a bounded region, learning is possible, namely an implementation of the Multiplicative Weights Update Algorithm (MWUA) using a discretized grid achieves a regret bound of $\tilde{O}(\sqrt{T})$ in expectation. We also present an online-to-offline reduction that shows that an efficient no-regret online algorithm (despite being allowed to choose a different set of candidate centres at each round) implies an offline efficient algorithm for the k-means problem, which is known to be NP-hard. In light of this hardness, we consider the slightly weaker requirement of comparing regret with respect to $(1 + \epsilon)OPT$ and present a no-regret algorithm with runtime $O\left(T \mathrm{poly}(\log(T),k,d,1/\epsilon)^{O(kd)}\right)$. Our algorithm is based on maintaining a set of points of bounded size which is a coreset that helps identifying the \emph{relevant} regions of the space for running an adaptive, more efficient, variant of the MWUA. We show that simpler online algorithms, such as \emph{Follow The Leader} (FTL), fail to produce sublinear regret in the worst case. We also report preliminary experiments with synthetic and real-world data. Our theoretical results answer an open question of Dasgupta (2008).},
keywords={mine}
}



@article{guedj2019free,
title={Still no free lunches: the price to pay for tighter {PAC-Bayes} bounds},
author={Benjamin Guedj and Louis Pujol},
year={2021},
journal = {Entropy},
volume = {23},
number = {11},
ARTICLE-NUMBER = {1529},
ISSN = {1099-4300},
doi = {10.3390/e23111529},
issuetitle = {Approximate Bayesian Inference},
abstract = {“No free lunch” results state the impossibility of obtaining meaningful bounds on the error of a learning algorithm without prior assumptions and modelling, which is more or less realistic for a given problem. Some models are “expensive” (strong assumptions, such as sub-Gaussian tails), others are “cheap” (simply finite variance). As it is well known, the more you pay, the more you get: in other words, the most expensive models yield the more interesting bounds. Recent advances in robust statistics have investigated procedures to obtain tight bounds while keeping the cost of assumptions minimal. The present paper explores and exhibits what the limits are for obtaining tight probably approximately correct (PAC)-Bayes bounds in a robust setting for cheap models.},
url = {https://www.mdpi.com/1099-4300/23/11/1529},
url_arXiv = "https://arxiv.org/abs/1910.04460",
url_PDF = "https://www.mdpi.com/1099-4300/23/11/1529/pdf",
eprint={1910.04460},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{nozawa2019pacbayesian,
title={{PAC-Bayesian} Contrastive Unsupervised Representation Learning},
author={Kento Nozawa and Pascal Germain and Benjamin Guedj},
year={2020},
abstract = "Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empirical successes recently, theoretical understanding of its performance was still missing. In a recent work, Arora et al. (2019) provide the first generalisation bounds for CURL, relying on a Rademacher complexity. We extend their framework to the flexible PAC-Bayes setting, allowing us to deal with the non-iid setting. We present PAC-Bayesian generalisation bounds for CURL, which are then used to derive a new representation learning algorithm. Numerical experiments on real-life datasets illustrate that our algorithm achieves competitive accuracy, and yields non-vacuous generalisation bounds.",
booktitle = {{Conference on Uncertainty in Artificial Intelligence [UAI]}},
url = "https://proceedings.mlr.press/v124/nozawa20a.html",
url_arXiv = "https://arxiv.org/abs/1910.04464",
url_PDF = "http://proceedings.mlr.press/v124/nozawa20a/nozawa20a.pdf",
url_Supplementary = "http://proceedings.mlr.press/v124/nozawa20a/nozawa20a-supp.pdf",
url_Code = "https://github.com/nzw0301/pb-contrastive",
url_Video = "https://youtu.be/WUh3Fgo5nhY",
eprint={1910.04464},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@article{dewez2020industrywide,
title={From industry-wide parameters to aircraft-centric on-flight inference: improving aeronautics performance prediction with machine learning},
author={Florent Dewez and Benjamin Guedj and Vincent Vandewalle},
year={2020},
journal={Data-Centric Engineering},
publisher={Cambridge University Press},
abstract = "Aircraft performance models play a key role in airline operations, especially in planning a fuel-efficient flight. In practice, manufacturers provide guidelines calibrated on one single aircraft, with performance modelling for all similar aircrafts (\emph{i.e.} same model) relying solely on that. In particular, it may poorly reflect on the current performance of a given aircraft. However, for each aircraft, flight data are continuously recorded and as such, not used to improve on the existing models. The key contribution of the present article is to foster the use of machine learning to leverage the massive amounts of collected data and update the models to reflect the actual performance of the aircraft. We illustrate our approach by focusing on the estimation of the drag and lift coefficients from recorded flight data. As these coefficients are not directly recorded, we resort to aerodynamics approximations. As a safety check, we provide bounds to assess the accuracy of both the aerodynamics approximation and the statistical performance of our approach. We provide numerical results on a collection of machine learning algorithms. We report excellent accuracy on real-life data and exhibit empirical evidence to support our modelling, in coherence with aerodynamics principles.",
eprint={2005.05286},
volume = "1",
doi = {10.1017/dce.2020.12},
url = "https://www.cambridge.org/core/journals/data-centric-engineering/article/from-industrywide-parameters-to-aircraftcentric-onflight-inference-improving-aeronautics-performance-prediction-with-machine-learning/7A5662351D23A3D855E7FBC58B45AB6D",
url_arXiv = "https://arxiv.org/abs/2005.05286",
url_PDF = "https://arxiv.org/pdf/2005.05286.pdf",
archivePrefix={arXiv},
primaryClass={stat.AP},
keywords={mine}
}

@article{Guedj_2020,
title={Kernel-Based Ensemble Learning in {Python}},
volume={11},
ISSN={2078-2489},
DOI={10.3390/info11020063},
url={http://dx.doi.org/10.3390/info11020063},
url_arXiv = "https://arxiv.org/abs/1912.08311",
url_PDF = "https://www.mdpi.com/2078-2489/11/2/63/pdf",
url_Code = "https://github.com/bhargavvader/pycobra",
number={2},
journal={Information},
publisher={MDPI AG},
author={Guedj, Benjamin and Srinivasa Desikan, Bhargav},
year={2020},
abstract = {We propose a new supervised learning algorithm for classification and regression problems where two or more preliminary predictors are available. We introduce KernelCobra, a non-linear learning strategy for combining an arbitrary number of initial predictors. KernelCobra builds on the COBRA algorithm introduced by Biau et al. (2016), which combined estimators based on a notion of proximity of predictions on the training data. While the COBRA algorithm used a binary threshold to declare which training data were close and to be used, we generalise this idea by using a kernel to better encapsulate the proximity information. Such a smoothing kernel provides more representative weights to each of the training points which are used to build the aggregate and final predictor, and KernelCobra systematically outperforms the COBRA algorithm. While COBRA is intended for regression, KernelCobra deals with classification and regression. KernelCobra is included as part of the open source Python package Pycobra (0.2.4 and onward), introduced by Srinivasa Desikan (2018). Numerical experiments were undertaken to assess the performance (in terms of pure prediction and computational complexity) of KernelCobra on real-life and synthetic datasets.},
month={Jan},
pages={63},
keywords={mine}
}

@article{haddouche2020pacbayes,
title={{PAC-Bayes} unleashed: generalisation bounds with unbounded losses},
author={Maxime Haddouche and Benjamin Guedj and Omar Rivasplata and John Shawe-Taylor},
year={2021},
eprint={2006.07279},
archivePrefix={arXiv},
journal = {Entropy},
volume = {23},
number = {10},
ARTICLE-NUMBER = {1330},
ISSN = {1099-4300},
DOI = {10.3390/e23101330},
issuetitle = {Approximate Bayesian Inference},
abstract = "We present new PAC-Bayesian generalisation bounds for learning problems with unbounded loss functions. This extends the relevance and applicability of the PAC-Bayes learning framework, where most of the existing literature focuses on supervised learning problems with a bounded loss function (typically assumed to take values in the interval [0;1]). In order to relax this assumption, we propose a new notion called HYPE (standing for \emph{HYPothesis-dependent rangE}), which effectively allows the range of the loss to depend on each predictor. Based on this new notion we derive a novel PAC-Bayesian generalisation bound for unbounded loss functions, and we instantiate it on a linear regression problem. To make our theory usable by the largest audience possible, we include discussions on actual computation, practicality and limitations of our assumptions.",
primaryClass={stat.ML},
url = "https://www.mdpi.com/1099-4300/23/10/1330",
url_arXiv = "https://arxiv.org/abs/2006.07279",
url_PDF = "https://arxiv.org/pdf/2006.07279.pdf",
url_Code = "https://github.com/bguedj/pac-bayes-self-bounding",
keywords={mine}
}

@inproceedings{vendeville2020opinions,
title={Towards control of opinion diversity by introducing zealots into a polarised social group},
author={Antoine Vendeville and Benjamin Guedj and Shi Zhou},
year={2022},
editor="Benito, Rosa Maria
and Cherifi, Chantal
and Cherifi, Hocine
and Moro, Esteban
and Rocha, Luis M.
and Sales-Pardo, Marta",
publisher="Springer International Publishing",
pages="341--352",
booktitle="Complex Networks {\&} Their Applications X",
abstract = {We explore a method to influence or even control the diversity of opinions within a polarised social group. We leverage the voter model in which users hold binary opinions and repeatedly update their beliefs based on others they connect with. Stubborn agents who never change their minds ("zealots") are also disseminated through the network, which is modelled by a connected graph. Building on earlier results, we provide a closed-form expression for the average opinion of the group at equilibrium. This leads us to a strategy to inject zealots into a polarised network in order to shift the average opinion towards any target value. We account for the possible presence of a backfire effect, which may lead the group to react negatively and reinforce its level of polarisation in response. Our results are supported by numerical experiments on synthetic data.},
isbn="978-3-030-93413-2",
url = "https://link.springer.com/chapter/10.1007%2F978-3-030-93413-2_29",
url_arXiv = "https://arxiv.org/abs/2006.07265",
url_PDF = "https://arxiv.org/pdf/2006.07265.pdf",
url_Code = "https://github.com/AntoineVendeville/HowOpinionsCrystallise",
doi = {10.1007/978-3-030-93413-2_29},
eprint={2006.07265},
archivePrefix={arXiv},
primaryClass={cs.SI},
keywords={mine}
}

@article{biggs2020differentiable,
title={Differentiable {PAC-Bayes} Objectives with Partially Aggregated Neural Networks},
author={Felix Biggs and Benjamin Guedj},
year={2021},
journal = {Entropy},
volume = {23},
number = {10},
ARTICLE-NUMBER = {1280},
ISSN = {1099-4300},
issuetitle = {Approximate Bayesian Inference},
eprint={2006.12228},
abstract = "We make two related contributions motivated by the challenge of training stochastic neural networks, particularly in a PAC–Bayesian setting: (1) we show how averaging over an ensemble of stochastic neural networks enables a new class of partially-aggregated estimators, proving that these lead to unbiased lower-variance output and gradient estimators; (2) we reformulate a PAC–Bayesian bound for signed-output networks to derive in combination with the above a directly optimisable, differentiable objective and a generalisation guarantee, without using a surrogate loss or loosening the bound. We show empirically that this leads to competitive generalisation guarantees and compares favourably to other methods for training such networks. Finally, we note that the above leads to a simpler PAC–Bayesian training scheme for sign-activation networks than previous work.",
url = "https://www.mdpi.com/1099-4300/23/10/1280",
url_arXiv = "https://arxiv.org/abs/2006.12228",
url_PDF = "https://arxiv.org/pdf/2006.12228.pdf",
archivePrefix={arXiv},
doi = "10.3390/e23101280",
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{mhammedi2020pacbayesian,
author    = {Zakaria Mhammedi and
Benjamin Guedj and
Robert C. Williamson},
editor    = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
title = {{PAC-Bayesian} Bound for the Conditional Value at Risk},
booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems [NeurIPS] 2020, December
6-12, 2020, virtual},
year      = {2020},
url       = {https://proceedings.neurips.cc/paper/2020/hash/d02e9bdc27a894e882fa0c9055c99722-Abstract.html},
timestamp = {Tue, 19 Jan 2021 15:57:31 +0100},
biburl    = {https://dblp.org/rec/conf/nips/MhammediGW20.bib},
bibsource = {dblp computer science bibliography, https://dblp.org},
eprint={2006.14763},
abstract = {Conditional Value at Risk (CVaR) is a family of "coherent risk measures" which generalize the traditional mathematical expectation. Widely used in mathematical finance, it is garnering increasing interest in machine learning, e.g., as an alternate approach to regularization, and as a means for ensuring fairness. This paper presents a generalization bound for learning algorithms that minimize the CVaR of the empirical loss. The bound is of PAC-Bayesian type and is guaranteed to be small when the empirical CVaR is small. We achieve this by reducing the problem of estimating CVaR to that of merely estimating an expectation. This then enables us, as a by-product, to obtain concentration inequalities for CVaR even when the random variable in question is unbounded.},
archivePrefix={arXiv},
url = "https://proceedings.neurips.cc/paper/2020/hash/d02e9bdc27a894e882fa0c9055c99722-Abstract.html",
url_arXiv = "https://arxiv.org/abs/2006.14763",
url_PDF = "https://proceedings.neurips.cc/paper/2020/file/d02e9bdc27a894e882fa0c9055c99722-Paper.pdf",
url_Supplementary = "https://proceedings.neurips.cc/paper/2020/file/d02e9bdc27a894e882fa0c9055c99722-Supplemental.pdf",
primaryClass={cs.LG},
keywords={mine}
}

@article{leroy2020magma,
title={{MAGMA}: Inference and Prediction with Multi-Task {Gaussian} Processes},
author={Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey},
year={2022},
journal = {Machine Learning},
abstract = {A novel multi-task Gaussian process (GP) framework is proposed, by using a common mean process for sharing information across tasks. In particular, we investigate the problem of time series forecasting, with the objective to improve multiple-step-ahead predictions. The common mean process is defined as a GP for which the hyper-posterior distribution is tractable. Therefore an EM algorithm is derived for handling both hyper-parameters optimisation and hyper-posterior computation. Unlike previous approaches in the literature, the model fully accounts for uncertainty and can handle irregular grids of observations while maintaining explicit formulations, by modelling the mean process in a unified GP framework. Predictive analytical equations are provided, integrating information shared across tasks through a relevant prior mean. This approach greatly improves the predictive performances, even far from observations, and may reduce significantly the computational complexity compared to traditional multi-task GP models. Our overall algorithm is called MAGMA (standing for Multi tAsk GPs with common MeAn). The quality of the mean process estimation, predictive performances, and comparisons to alternatives are assessed in various simulated scenarios and on real datasets.},
eprint={2007.10731},
archivePrefix={arXiv},
primaryClass={stat.CO},
url = {https://arxiv.org/abs/2007.10731},
url_PDF = {https://arxiv.org/pdf/2007.10731.pdf},
url_Code = {https://github.com/ArthurLeroy/MAGMA},
DOI = "10.1007/s10994-022-06172-1",
keywords={mine}
}

@article{vendeville2020elections,
title={Forecasting elections results via the voter model with stubborn nodes},
author={Antoine Vendeville and Benjamin Guedj and Shi Zhou},
year={2021},
volume = {6},
journal = {Applied Network Science},
abstract = {In this paper we propose a novel method to forecast the result of elections using only official results of previous ones. It is based on the voter model with stubborn nodes and uses theoretical results developed in a previous work of ours. We look at popular vote shares for the Conservative and Labour parties in the UK and the Republican and Democrat parties in the US. We are able to perform time-evolving estimates of the model parameters and use these to forecast the vote shares for each party in any election. We obtain a mean absolute error of 4.74\%. As a side product, our parameters estimates provide meaningful insight on the political landscape, informing us on the proportion of voters that are strong supporters of each of the considered parties.},
url = "https://appliednetsci.springeropen.com/articles/10.1007/s41109-020-00342-7",
url_arXiv = "https://arxiv.org/abs/2009.10627",
url_PDF = "https://arxiv.org/pdf/2009.10627.pdf",
url_Code = "https://github.com/AntoineVendeville/HowOpinionsCrystallise",
DOI = {10.1007/s41109-020-00342-7},
eprint={2009.10627},
archivePrefix={arXiv},
primaryClass={cs.SI},
keywords={mine}
}

@article{leroy2020clusterspecific,
title={Cluster-Specific Predictions with Multi-Task {Gaussian} Processes}, 
author={Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey},
year={2023},
journal = {Journal of Machine Learning Research [JMLR]},
abstract = {A model involving Gaussian processes (GPs) is introduced to simultaneously handle multitask learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors’ estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty in both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performance when dealing with group-structured data. The model handles irregular grids of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real data sets. The overall algorithm, called MagmaClust, is publicly available as an R package.},
volume  = {24},
number  = {5},
pages   = {1--49},
url = "https://jmlr.org/papers/v24/20-1321.html",
url_arXiv = "https://arxiv.org/abs/2011.07866",
url_PDF = "https://jmlr.org/papers/volume24/20-1321/20-1321.pdf",
url_Code = "https://github.com/ArthurLeroy/MagmaClustR",
eprint={2011.07866},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@article{dewez2020endtoend,
title={An end-to-end data-driven optimisation framework for constrained trajectories}, 
author={Florent Dewez and Benjamin Guedj and Arthur Talpaert and Vincent Vandewalle},
year={2022},
journal={Data-Centric Engineering},
publisher={Cambridge University Press},
volume = "3",
abstract = {Many real-world problems require to optimize trajectories under constraints. Classical approaches are often based on optimal control methods but require an exact knowledge of the underlying dynamics and constraints, which could be challenging or even out of reach. In view of this, we leverage data-driven approaches to design a new end-to-end framework which is dynamics-free for optimized and realistic trajectories. Trajectories are here decomposed on function basis, trading the initial infinite dimension problem on a multivariate functional space for a parameter optimization problem. Then a maximum a posteriori approach which incorporates information from data is used to obtain a new penalized optimization problem. The penalized term narrows the search on a region centered on data and includes estimated features of the problem. We apply our data-driven approach to two settings in aeronautics and sailing routes optimization. The developed approach is implemented in the Python library PyRotor.},
url = "https://www.cambridge.org/core/journals/data-centric-engineering/article/an-endtoend-datadriven-optimization-framework-for-constrained-trajectories/B4BF729E1681F795FFE79A1F4B544CE5",
url_arXiv = "https://arxiv.org/abs/2011.11820",
url_PDF = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/B4BF729E1681F795FFE79A1F4B544CE5/S2632673622000065a.pdf/an-end-to-end-data-driven-optimization-framework-for-constrained-trajectories.pdf",
url_Code = "https://github.com/bguedj/pyrotor",
eprint={2011.11820},
archivePrefix={arXiv},
DOI = "10.1017/dce.2022.6",
primaryClass={stat.AP},
keywords={mine}
}



@techreport{brotcorne:hal-03033764,
title = {Indicateurs de suivi de l'activit{\'e} scientifique de l'{Inria}},
author = {Brotcorne, Luce and Canteaut, Anne and Viana, Aline Carneiro and Grandmont, C{\'e}line and Guedj, Benjamin and Huot, St{\'e}phane and Issarny, Val{\'e}rie and Pallez, Guillaume and Perrier, Val{\'e}rie and Quema, Vivien and Pomet, Jean-Baptiste and Rival, Xavier and Salvati, Sylvain and Thom{\'e}, Emmanuel},
url = {https://hal.inria.fr/hal-03033764},
type = {Research Report},
institution = {{Inria}},
abstract = {La Commission d'Evaluation de l'Inria a mis en place début 2020, à la demande de la Direction Générale, un groupe de travail visant à réfléchir à des indicateurs pour l'analyse qualitative de l'activité scientifique de l'Institut. Ce document est le fruit des discussions de ce groupe, enrichies par les échanges avec la Direction Générale d'Inria et avec l'ensemble de la Commision d'Evaluation. Au-delà de cette « commande », il nous a semblé important de saisir cette opportunité pour mener une réflexion collective, qui dépasse le cadre de notre Institut, sur les indicateurs, et sur ce qu'ils peuvent ou ne peuvent pas dire de nos pratiques scientifiques.},
year = {2020},
month = Dec,
url_PDF = {https://hal.inria.fr/hal-03033764/file/CE_GT-Suivi.pdf},
keywords={mine}
}

@unpublished{cantelobre2020pacbayesian,
title={A {PAC-Bayesian} Perspective on Structured Prediction with Implicit Loss Embeddings}, 
author={Théophile Cantelobre and Benjamin Guedj and Mar\'ia P\'erez-Ortiz and John Shawe-Taylor},
year={2020},
note = "Submitted.",
abstract = {Many practical machine learning tasks can be framed as Structured prediction problems, where several output variables are predicted and considered interdependent. Recent theoretical advances in structured prediction have focused on obtaining fast rates convergence guarantees, especially in the Implicit Loss Embedding (ILE) framework. PAC-Bayes has gained interest recently for its capacity of producing tight risk bounds for predictor distributions. This work proposes a novel PAC-Bayes perspective on the ILE Structured prediction framework. We present two generalization bounds, on the risk and excess risk, which yield insights into the behavior of ILE predictors. Two learning algorithms are derived from these bounds. The algorithms are implemented and their behavior analyzed, with source code available at \url{https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction}},
url = {https://arxiv.org/abs/2012.03780},
url_PDF = {https://arxiv.org/pdf/2012.03780.pdf},
url_Code = {https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction},
eprint={2012.03780},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@unpublished{haddouche2020upper,
title={Upper and Lower Bounds on the Performance of {Kernel PCA}}, 
author={Maxime Haddouche and Benjamin Guedj and Omar Rivasplata and John Shawe-Taylor},
year={2020},
note = "Submitted.",
abstract = {Principal Component Analysis (PCA) is a popular method for dimension reduction and has attracted an unfailing interest for decades. Recently, kernel PCA has emerged as an extension of PCA but, despite its use in practice, a sound theoretical understanding of kernel PCA is missing. In this paper, we contribute lower and upper bounds on the efficiency of kernel PCA, involving the empirical eigenvalues of the kernel Gram matrix. Two bounds are for fixed estimators, and two are for randomized estimators through the PAC-Bayes theory. We control how much information is captured by kernel PCA on average, and we dissect the bounds to highlight strengths and limitations of the kernel PCA algorithm. Therefore, we contribute to the better understanding of kernel PCA. Our bounds are briefly illustrated on a toy numerical example.},
url = {https://arxiv.org/abs/2012.10369},
url_PDF = {https://arxiv.org/pdf/2012.10369.pdf},
eprint={2012.10369},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@techreport{guedj:hal-03277494,
TITLE = {{Covid-19} and {AI}: Unexpected Challenges and Lessons},
AUTHOR = {Guedj, Benjamin},
url = {https://hal.inria.fr/hal-03277494},
YEAR = {2021},
MONTH = {July},
type = {Research Report},
abstract = {On May 21st, 2021, we held the webinar "Covid-19 and AI: unexpected challenges and lessons". This short note presents its highlights.},
url_PDF = {https://hal.inria.fr/hal-03277494v2/file/main.pdf},
url_Video = {https://youtu.be/5AnQ35tBnv0},
keywords={mine}
}

@inproceedings{zantedeschi2021learning,
title={Learning Stochastic Majority Votes by Minimizing a {PAC-Bayes} Generalization Bound},
author={Valentina Zantedeschi and Paul Viallard and Emilie Morvant and Rémi Emonet and Amaury Habrard and Pascal Germain and Benjamin Guedj},
year={2021},
editor    = {Alina Beygelzimer and Percy Liang and Jenn Wortman Vaughan and Yann Dauphin},
booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems [NeurIPS] 2021},
pages     = {},
abstract = {We investigate a stochastic counterpart of majority votes over finite ensembles of classifiers, and study its generalization properties. While our approach holds for arbitrary distributions, we instantiate it with Dirichlet distributions: this allows for a closed-form and differentiable expression for the expected risk, which then turns the generalization bound into a tractable training objective.The resulting stochastic majority vote learning algorithm achieves state-of-the-art accuracy and benefits from (non-vacuous) tight generalization bounds, in a series of numerical experiments when compared to competing algorithms which also minimize PAC-Bayes objectives -- both with uninformed (data-independent) and informed (data-dependent) priors.},
url = {https://proceedings.neurips.cc/paper/2021/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html},
url_arXiv = {https://arxiv.org/abs/2106.12535},
url_PDF = {https://proceedings.neurips.cc/paper/2021/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf},
url_Supplementary = {https://proceedings.neurips.cc/paper/2021/file/0415740eaa4d9decbc8da001d3fd805f-Supplemental.pdf},
url_Code = {https://github.com/vzantedeschi/StocMV},
url_Video = {https://nips.cc/virtual/2021/poster/28787},
eprint={2106.12535},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{biggs2021margins,
title={On Margins and Derandomisation in {PAC-Bayes}},
author={Felix Biggs and Benjamin Guedj},
year={2022},
pages = 	 {3709--3731},
editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
volume = 	 {151},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {28--30 Mar},
publisher =    {PMLR},
booktitle={Proceedings of The 25th International Conference on Artificial Intelligence and Statistics [AISTATS]},
abstract = {We give a general recipe for derandomising PAC-Bayesian bounds using margins, with the critical ingredient being that our randomised predictions concentrate around some value. The tools we develop straightforwardly lead to margin bounds for various classifiers, including linear prediction—a class that includes boosting and the support vector machine—single-hidden-layer neural networks with an unusual erf activation function, and deep ReLU networks. Further we extend to partially-derandomised predictors where only some of the randomness of our estimators is removed, letting us extend bounds to cases where the concentration properties of our estimators are otherwise poor.},
url = {https://proceedings.mlr.press/v151/biggs22a.html},
url_arXiv = {https://arxiv.org/abs/2107.03955},
url_PDF = {https://proceedings.mlr.press/v151/biggs22a/biggs22a.pdf},
url_Code = {https://github.com/biggs/margins-and-derandomisation-code},
url_Video = {https://virtual.aistats.org/virtual/2022/poster/3295},
eprint={2107.03955},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@unpublished{perezortiz2021learning,
title={Learning {PAC-Bayes} Priors for Probabilistic Neural Networks},
author={Mar\'ia P\'erez-Ortiz and Omar Rivasplata and Benjamin Guedj and Matthew Gleeson and Jingyu Zhang and John Shawe-Taylor and Miroslaw Bober and Josef Kittler},
year={2021},
note = "Submitted.",
abstract = {Recent works have investigated deep learning models trained by optimising PAC-Bayes bounds, with priors that are learnt on subsets of the data. This combination has been shown to lead not only to accurate classifiers, but also to remarkably tight risk certificates, bearing promise towards self-certified learning (i.e. use all the data to learn a predictor and certify its quality). In this work, we empirically investigate the role of the prior. We experiment on 6 datasets with different strategies and amounts of data to learn data-dependent PAC-Bayes priors, and we compare them in terms of their effect on test performance of the learnt predictors and tightness of their risk certificate. We ask what is the optimal amount of data which should be allocated for building the prior and show that the optimum may be dataset dependent. We demonstrate that using a small percentage of the prior-building data for validation of the prior leads to promising results. We include a comparison of underparameterised and overparameterised models, along with an empirical study of different training objectives and regularisation strategies to learn the prior distribution.},
url = {https://arxiv.org/abs/2109.10304},
url_PDF = {https://arxiv.org/pdf/2109.10304.pdf},
eprint={2109.10304},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@article{schrab2021mmd,
title={{MMD} Aggregated Two-Sample Test},
author={Antonin Schrab and Ilmun Kim and Mélisande Albert and Béatrice Laurent and Benjamin Guedj and Arthur Gretton},
year={2021},
journal = {Journal of Machine Learning Research [JMLR]},
note = "Accepted for publication pending minor revision",
abstract = {We propose a novel nonparametric two-sample test based on the Maximum Mean Discrepancy (MMD), which is constructed by aggregating tests with different kernel bandwidths. This aggregation procedure, called MMDAgg, ensures that test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power), or arbitrary kernel choices such as the median heuristic. We work in the non-asymptotic framework, and prove that our aggregated test is minimax adaptive over Sobolev balls. Our guarantees are not restricted to a specific kernel, but hold for any product of one-dimensional translation invariant characteristic kernels which are absolutely and square integrable. Moreover, our results apply for popular numerical procedures to determine the test threshold, namely permutations and the wild bootstrap. Through numerical experiments on both synthetic and real-world datasets, we demonstrate that MMDAgg outperforms alternative state-of-the-art approaches to MMD kernel adaptation for two-sample testing.},
url = {https://arxiv.org/abs/2110.15073},
url_PDF = {https://arxiv.org/pdf/2110.15073.pdf},
url_Code = {https://github.com/antoninschrab/mmdagg-paper},
url_Slides = {https://antoninschrab.github.io/files/Slides-31-05-22.pdf},
url_Poster = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
url_Posterbis = {https://antoninschrab.github.io/files/Simplified_Poster.pdf},
url_Video = {https://youtu.be/OWh6Hj10wsY},
eprint={2110.15073},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@inproceedings{perezortiz2021progress,
title={Progress in self-certified neural networks},
author={Mar\'ia P\'erez-Ortiz and Omar Rivasplata and Emilio Parrado-Hernandez and Benjamin Guedj and John Shawe-Taylor},
year={2021},
booktitle={{NeurIPS 2021 workshop Bayesian Deep Learning [BDL]}},
abstract = {A learning method is self-certified if it uses all available data to simultaneously learn a predictor and certify its quality with a statistical certificate that is valid on unseen data. Recent work has shown that neural network models trained by optimising PAC-Bayes bounds lead not only to accurate predictors, but also to tight risk certificates, bearing promise towards achieving self-certified learning. In this context, learning and certification strategies based on PAC-Bayes bounds are especially attractive due to their ability to leverage all data to learn a posterior and simultaneously certify its risk. In this paper, we assess the progress towards self-certification in probabilistic neural networks learnt by PAC-Bayes inspired objectives. We empirically compare (on 4 classification datasets) classical test set bounds for deterministic predictors and a PAC-Bayes bound for randomised self-certified predictors. We first show that both of these generalisation bounds are not too far from out-of-sample test set errors. We then show that in data starvation regimes, holding out data for the test set bounds adversely affects generalisation performance, while self-certified strategies based on PAC-Bayes bounds do not suffer from this drawback, proving that they might be a suitable choice for the small data regime. We also find that probabilistic neural networks learnt by PAC-Bayes inspired objectives lead to certificates that can be surprisingly competitive with commonly used test set bounds.},
url = "http://bayesiandeeplearning.org/2021/papers/38.pdf",
url_arXiv = {https://arxiv.org/abs/2111.07737},
url_PDF = {https://arxiv.org/pdf/2111.07737.pdf},
eprint={2111.07737},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{cherief2021vae,
title={On {PAC-Bayesian} reconstruction guarantees for {VAEs}},
author={Ch\'erief-Abdellatif, Badr-Eddine and Shi, Yuyang and Doucet, Arnaud and Guedj, Benjamin},
year={2022},
pages = 	 {3066--3079},
editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
volume = 	 {151},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {28--30 Mar},
publisher =    {PMLR},
booktitle={Proceedings of The 25th International Conference on Artificial Intelligence and Statistics [AISTATS]},
abstract = {Despite its wide use and empirical successes, the theoretical understanding and study of the behaviour and performance of the variational autoencoder (VAE) have only emerged in the past few years. We contribute to this recent line of work by analysing the VAE’s reconstruction ability for unseen test data, leveraging arguments from the PAC-Bayes theory. We provide generalisation bounds on the theoretical reconstruction error, and provide insights on the regularisation effect of VAE objectives. We illustrate our theoretical results with supporting experiments on classical benchmark datasets.},
url = {https://proceedings.mlr.press/v151/cherief-abdellatif22a.html},
url_arXiv = {https://arxiv.org/abs/2202.11455},
url_PDF = {https://proceedings.mlr.press/v151/cherief-abdellatif22a/cherief-abdellatif22a.pdf},
url_Code = {https://github.com/yuyang-shi/PBVAE},
url_Video = {https://virtual.aistats.org/virtual/2022/poster/3265},
eprint={2202.11455},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{schrab2022ksd,
title={{KSD} Aggregated Goodness-of-fit Test},
author={Antonin Schrab and Benjamin Guedj and Arthur Gretton},
year={2022},
booktitle = {NeurIPS},
abstract = {We investigate properties of goodness-of-fit tests based on the Kernel Stein Discrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg, which aggregates multiple tests with different kernels. KSDAgg avoids splitting the data to perform kernel selection (which leads to a loss in test power), and rather maximises the test power over a collection of kernels. We provide theoretical guarantees on the power of KSDAgg: we show it achieves the smallest uniform separation rate of the collection, up to a logarithmic term. KSDAgg can be computed exactly in practice as it relies either on a parametric bootstrap or on a wild bootstrap to estimate the quantiles and the level corrections. In particular, for the crucial choice of bandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such as median or standard deviation) or to data splitting. We find on both synthetic and real-world data that KSDAgg outperforms other state-of-the-art adaptive KSD-based goodness-of-fit testing procedures.},
url = {https://arxiv.org/abs/2202.00824},
url_PDF = {https://arxiv.org/pdf/2202.00824.pdf},
url_Code = {https://github.com/antoninschrab/ksdagg-paper},
url_Slides = {https://antoninschrab.github.io/files/Slides-31-05-22.pdf},
url_Poster = {https://antoninschrab.github.io/files/Poster_MMDAgg_KSDAgg.pdf},
url_Posterbis = {https://antoninschrab.github.io/files/Simplified_Poster.pdf},
url_Video = {https://www.youtube.com/watch?v=OWh6Hj10wsY},
eprint={2202.00824},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@inproceedings{biggs2022shallow,
booktitle={Proceedings of the 39th International Conference on Machine Learning [ICML]},
title={Non-Vacuous Generalisation Bounds for Shallow Neural Networks},
author={Felix Biggs and Benjamin Guedj},
year={2022},
pages = 	 {1963--1981},
editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
volume = 	 {162},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {July},
publisher =  {PMLR},
abstract = {We focus on a specific class of shallow neural networks with a single hidden layer, namely those with $L_2$-normalised data and either a sigmoid-shaped Gaussian error function (“erf”) activation or a Gaussian Error Linear Unit (GELU) activation. For these networks, we derive new generalisation bounds through the PAC-Bayesian theory; unlike most existing such bounds they apply to neural networks with deterministic rather than randomised parameters. Our bounds are empirically non-vacuous when the network is trained with vanilla stochastic gradient descent on MNIST and Fashion-MNIST.},
url = {https://proceedings.mlr.press/v162/biggs22a.html},
url_arXiv = {https://arxiv.org/abs/2202.01627},
url_PDF = {https://proceedings.mlr.press/v162/biggs22a/biggs22a.pdf},
url_Code = {},
url_Video = {https://icml.cc/virtual/2022/spotlight/17948},
url_Slides = {https://icml.cc/media/icml-2022/Slides/17948.pdf},
url_Poster = {https://icml.cc/media/PosterPDFs/ICML%202022/194cf6c2de8e00c05fcf16c498adc7bf.png},
url_SlidesLive = {https://slideslive.com/38983983/nonvacuous-generalisation-bounds-for-shallow-neural-networks},
eprint={2202.01627},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{cantelobre2022dissimilarity,
booktitle={Proceedings of the 39th International Conference on Machine Learning [ICML]},
title={Measuring dissimilarity with diffeomorphism invariance},
author={Cantelobre, Th{\'e}ophile and Ciliberto, Carlo and Guedj, Benjamin and Rudi, Alessandro},
year={2022},
pages = 	 {2572--2596},
editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
volume = 	 {162},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {17--23 Jul},
publisher =  {PMLR},
abstract = {Measures of similarity (or dissimilarity) are a key ingredient to many machine learning algorithms. We introduce DID, a pairwise dissimilarity measure applicable to a wide range of data spaces, which leverages the data’s internal structure to be invariant to diffeomorphisms. We prove that DID enjoys properties which make it relevant for theoretical study and practical use. By representing each datum as a function, DID is defined as the solution to an optimization problem in a Reproducing Kernel Hilbert Space and can be expressed in closed-form. In practice, it can be efficiently approximated via Nystr{ö}m sampling. Empirical experiments support the merits of DID.},
url = 	 {https://proceedings.mlr.press/v162/cantelobre22a.html},
url_arXiv = {https://arxiv.org/abs/2202.05614},
url_PDF = {https://proceedings.mlr.press/v162/cantelobre22a/cantelobre22a.pdf},
url_Code = {https://github.com/theophilec/diffy},
url_Video = {https://youtu.be/WKADjgoTBBs},
url_Videobis = {https://icml.cc/virtual/2022/spotlight/17588},
url_Slides = {https://icml.cc/media/icml-2022/Slides/17588_MqtuoR9.pdf},
url_Poster = {https://icml.cc/media/PosterPDFs/ICML%202022/9ff0525c64bf3d4c9957a1d4397f1b40.png},
url_SlidesPoster = {https://icml.cc/media/icml-2022/Slides/17587.pdf},
url_SlidesLive = {https://slideslive.com/38983838/measuring-dissimilarity-with-diffeomorphism-invariance},
eprint={2202.05614},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@unpublished{adams2022confusion,
title={Controlling Confusion via Generalisation Bounds},
author={Reuben Adams and John Shawe-Taylor and Benjamin Guedj},
year={2022},
note = "Submitted.",
abstract = {We establish new generalisation bounds for multiclass classification by abstracting to a more general setting of discretised error types. Extending the PAC-Bayes theory, we are hence able to provide fine-grained bounds on performance for multiclass classification, as well as applications to other learning problems including discretisation of regression losses. Tractable training objectives are derived from the bounds. The bounds are uniform over all weightings of the discretised error types and thus can be used to bound weightings not foreseen at training, including the full confusion matrix in the multiclass classification case.},
url = {https://arxiv.org/abs/2202.05560},
url_PDF = {https://arxiv.org/pdf/2202.05560.pdf},
url_Video = {https://youtu.be/yMBv89kP42w},
eprint={2202.05560},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@unpublished{picard2022divergence,
title={On change of measure inequalities for $f$-divergences},
author={Antoine Picard-Weibel and Benjamin Guedj},
year={2022},
note = "Submitted.",
abstract = {We propose new change of measure inequalities based on $f$-divergences (of which the Kullback-Leibler divergence is a particular case). Our strategy relies on combining the Legendre transform of $f$-divergences and the Young-Fenchel inequality. By exploiting these new change of measure inequalities, we derive new PAC-Bayesian generalisation bounds with a complexity involving $f$-divergences, and holding in mostly unchartered settings (such as heavy-tailed losses). We instantiate our results for the most popular $f$-divergences.},
url = {https://arxiv.org/abs/2202.05568},
url_PDF = {https://arxiv.org/pdf/2202.05568.pdf},
eprint={2202.05568},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@unpublished{vendeville2022depolarising,
title={Depolarising Social Networks: Optimisation of Exposure to Adverse Opinions in the Presence of a Backfire Effect},
author={Antoine Vendeville and Benjamin Guedj and Shi Zhou},
year={2022},
note = "Submitted.",
abstract = {As social networks are ubiquitous in everyday life, problems such as misinformation, bots and polarisation of opinion gain more and more importance. This paper focuses on the last one as we propose novel methods to reduce the amount of polarisation in a social group. We leverage the voter model in which users hold binary opinions and repeatedly update their beliefs based on others they connect with. Stubborn agents who never change their minds ("zealots") are also disseminated through the network. We are interested in two equilibrium metrics, the diversity of opinions $\sigma$ and the density of active links $\rho$. The latter is a measure of the exposure to adverse opinions. We derive formulas to compute them that are valid in any directed, weighted network. Then we study the problem of finding optimal numbers of zealots in order to maximise these quantities. We account for the presence of a backfire effect, which may lead the group to react negatively and reinforce its level of polarisation in response. We provide exact solutions in the specific context of a complete unweighted graph, and propose a method for $\sigma$ in the general case. Finally we apply these problems to the network of the US House of Representatives. The network exhibits high levels of opinion diversity but lower levels of active links density. We find optimal numbers of zealots that maximise these quantities and show that both can be efficiently increased in some cases.},
url = {https://arxiv.org/abs/2203.02002},
url_PDF = {https://arxiv.org/pdf/2203.02002.pdf},
url_Code = {https://github.com/antoinevendeville/howopinionscrystallise},
eprint={2203.02002},
archivePrefix={arXiv},
primaryClass={cs.SI},
keywords={mine}
}

@unpublished{wei2022reprint,
title={Reprint: a randomized extrapolation based on principal components for data augmentation},
author={Jiale Wei and Qiyuan Chen and Pai Peng and Benjamin Guedj and Le Li},
year={2022},
note = "Submitted.",
abstract = {Data scarcity and data imbalance have attracted a lot of attention in many fields. Data augmentation, explored as an effective approach to tackle them, can improve the robustness and efficiency of classification models by generating new samples. This paper presents REPRINT, a simple and effective hidden-space data augmentation method for imbalanced data classification. Given hidden-space representations of samples in each class, REPRINT extrapolates, in a randomized fashion, augmented examples for target class by using subspaces spanned by principal components to summarize distribution structure of both source and target class. Consequently, the examples generated would diversify the target while maintaining the original geometry of target distribution. Besides, this method involves a label refinement component which allows to synthesize new soft labels for augmented examples. Compared with different NLP data augmentation approaches under a range of data imbalanced scenarios on four text classification benchmark, REPRINT shows prominent improvements. Moreover, through comprehensive ablation studies, we show that label refinement is better than label-preserving for augmented examples, and that our method suggests stable and consistent improvements in terms of suitable choices of principal components. Moreover, REPRINT is appealing for its easy-to-use since it contains only one hyperparameter determining the dimension of subspace and requires low computational resource.},
url = {https://arxiv.org/abs/2204.12024},
url_PDF = {https://arxiv.org/pdf/2204.12024.pdf},
url_Code = {https://github.com/bigdata-ccnu/REPRINT},
eprint={2204.12024},
archivePrefix={arXiv},
primaryClass={cs.CL},
keywords={mine}
}

@inproceedings{haddouche2022online,
title={Online PAC-Bayesian Learning},
author={Maxime Haddouche and Benjamin Guedj},
year={2022},
booktitle = {NeurIPS},
note = "Accepted for publication",
abstract = {Most PAC-Bayesian bounds hold in the batch learning setting where data is collected at once, prior to inference or prediction. This somewhat departs from many contemporary learning problems where data streams are collected and the algorithms must dynamically adjust. We prove new PAC-Bayesian bounds in this online learning framework, leveraging an updated definition of regret, and we revisit classical PAC-Bayesian results with a batch-to-online conversion, extending their remit to the case of dependent data. Our results hold for bounded losses, potentially \emph{non-convex}, paving the way to promising developments in online learning.},
url = {https://arxiv.org/abs/2206.00024},
url_PDF = {https://arxiv.org/pdf/2206.00024.pdf},
eprint={2206.00024},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{biggs2022majority,
title={On Margins and Generalisation for Voting Classifiers},
author={Felix Biggs and Valentina Zantedeschi and Benjamin Guedj},
year={2022},
booktitle = {NeurIPS},
note = "Accepted for publication",
abstract = {We study the generalisation properties of majority voting on finite ensembles of classifiers, proving margin-based generalisation bounds via the PAC-Bayes theory. These provide state-of-the-art guarantees on a number of classification tasks. Our central results leverage the Dirichlet posteriors studied recently by Zantedeschi et al. [2021] for training voting classifiers; in contrast to that work our bounds apply to non-randomised votes via the use of margins. Our contributions add perspective to the debate on the "margins theory" proposed by Schapire et al. [1998] for the generalisation of ensemble classifiers.},
url = {https://arxiv.org/abs/2206.04607},
url_PDF = {https://arxiv.org/pdf/2206.04607.pdf},
eprint={2206.04607},
archivePrefix={arXiv},
primaryClass={cs.LG},
keywords={mine}
}

@inproceedings{schrab2022ustats,
title={Efficient Aggregated Kernel Tests using Incomplete $U$-statistics},
author={Antonin Schrab and Ilmun Kim and Benjamin Guedj and Arthur Gretton},
year={2022},
booktitle = {NeurIPS},
note = "Accepted for publication",
abstract = {We propose a series of computationally efficient, nonparametric tests for the two-sample, independence and goodness-of-fit problems, using the Maximum Mean Discrepancy (MMD), Hilbert Schmidt Independence Criterion (HSIC), and Kernel Stein Discrepancy (KSD), respectively. Our test statistics are incomplete $U$-statistics, with a computational cost that interpolates between linear time in the number of samples, and quadratic time, as associated with classical $U$-statistic tests. The three proposed tests aggregate over several kernel bandwidths to detect departures from the null on various scales: we call the resulting tests MMDAggInc, HSICAggInc and KSDAggInc. For the test thresholds, we derive a quantile bound for wild bootstrapped incomplete $U$-statistics, which is of independent interest. We derive uniform separation rates for MMDAggInc and HSICAggInc, and quantify exactly the trade-off between computational efficiency and the attainable rates: this result is novel for tests based on incomplete $U$-statistics, to our knowledge. We further show that in the quadratic-time case, the wild bootstrap incurs no penalty to test power over more widespread permutation-based approaches, since both attain the same minimax optimal rates (which in turn match the rates that use oracle quantiles). We support our claims with numerical experiments on the trade-off between computational efficiency and test power. In the three testing frameworks, we observe that our proposed linear-time aggregated tests obtain higher power than current state-of-the-art linear-time kernel tests.},
url = {https://arxiv.org/abs/2206.09194},
url_PDF = {https://arxiv.org/pdf/2206.09194.pdf},
url_Slides = {https://antoninschrab.github.io/files/Slides-31-05-22.pdf},
url_Poster = {},
url_Posterbis = {https://antoninschrab.github.io/files/Simplified_Poster.pdf},
eprint={2206.09194},
archivePrefix={arXiv},
primaryClass={stat.ML},
keywords={mine}
}

@inproceedings{vendeville2022online,
title={Opening up echo chambers via optimal content recommendation},
author={Antoine Vendeville and Anastasios Giovanidis and Effrosyni Papanastasiou and Benjamin Guedj},
year={2023},
editor="Cherifi, Hocine
and Mantegna, Rosario Nunzio
and Rocha, Luis M.
and Cherifi, Chantal
and Miccich{\`e}, Salvatore",
booktitle = {Complex Networks and Their Applications XI},
abstract = {Online social platforms have become central in the political debate. In this context, the existence of echo chambers is a problem of primary relevance. These clusters of like-minded individuals tend to reinforce prior beliefs, elicit animosity towards others and aggravate the spread of misinformation. We study this phenomenon on a Twitter dataset related to the 2017 French presidential elections and propose a method to tackle it with content recommendations. We use a quadratic program to find optimal recommendations that maximise the diversity of content users are exposed to, while still accounting for their preferences. Our method relies on a theoretical model that can sufficiently describe how content flows through the platform. We show that the model provides good approximations of empirical measures and demonstrate the effectiveness of the optimisation algorithm at mitigating the echo chamber effect on this dataset, even with limited budget for recommendations.},
url = {https://link.springer.com/chapter/10.1007/978-3-031-21127-0_7},
url_arXiv = {https://arxiv.org/abs/2206.03859},
url_PDF = {https://arxiv.org/pdf/2206.03859.pdf},
eprint={2206.03859},
doi = {10.1007/978-3-031-21127-0_7},
archivePrefix={arXiv},
primaryClass={cs.SI},
isbn="978-3-031-21127-0",
publisher="Springer International Publishing",
address="Cham",
pages="74--85",
keywords={mine}
}

@unpublished{clerico2022pacbayes,
title = {A {PAC-Bayes} bound for deterministic classifiers},
author = {Clerico, Eugenio and Deligiannidis, George and Guedj, Benjamin and Doucet, Arnaud},
year = {2022},
note = "Submitted.",
abstract = {We establish a disintegrated PAC-Bayesian bound, for classifiers that are trained via continuous-time (non-stochastic) gradient descent. Contrarily to what is standard in the PAC-Bayesian setting, our result applies to a training algorithm that is deterministic, conditioned on a random initialisation, without requiring any \emph{de-randomisation} step. We provide a broad discussion of the main features of the bound that we propose, and we study analytically and empirically its behaviour on linear models, finding promising results.},
url = {https://arxiv.org/abs/2209.02525},
url_PDF = {https://arxiv.org/pdf/2209.02525.pdf},
doi = {10.48550/ARXIV.2209.02525},
eprint={2209.02525},
archivePrefix={arXiv},
primaryClass={stat.ML},
copyright = {Creative Commons Attribution 4.0 International},
keywords={mine}
}

@unpublished{haddouche2022supermartingales,
title = {{PAC-Bayes} with Unbounded Losses through Supermartingales},
author = {Haddouche, Maxime and Guedj, Benjamin},
year = {2022},
note = "Submitted.",
abstract = {While PAC-Bayes is now an established learning framework for bounded losses, its extension to the case of unbounded losses (as simple as the squared loss on an unbounded space) remains largely uncharted and has attracted a growing interest in recent years. We contribute to this line of work by developing an extention of Markov's inequality for supermartingales, which we use to establish a novel PAC-Bayesian generalisation bound holding for unbounded losses. We show that this bound extends, unifies and even improves on existing PAC-Bayesian bounds.},
url = {https://arxiv.org/abs/2210.00928},
url_PDF = {https://arxiv.org/pdf/2210.00928.pdf},
doi = {10.48550/ARXIV.2210.00928},
eprint={2210.00928},
archivePrefix={arXiv},
primaryClass={stat.ML},
copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
keywords={mine}
}

@inproceedings{biggs2022examples,
title = {Tighter {PAC-Bayes} Generalisation Bounds by Leveraging Example Difficulty},
author = {Biggs, Felix and Guedj, Benjamin},
year = {2023},
booktitle={Proceedings of The 26th International Conference on Artificial Intelligence and Statistics [AISTATS]},
note = "Accepted for publication",
abstract = {We introduce a modified version of the excess risk, which can be used to obtain tighter, fast-rate PAC-Bayesian generalisation bounds. This modified excess risk leverages information about the relative hardness of data examples to reduce the variance of its empirical counterpart, tightening the bound. We combine this with a new bound for $[−1,1]$-valued (and potentially non-independent) signed losses, which is more favourable when they empirically have low variance around $0$. The primary new technical tool is a novel result for sequences of interdependent random vectors which may be of independent interest. We empirically evaluate these new bounds on a number of real-world datasets.},
url = {https://arxiv.org/abs/2210.11289},
url_PDF = {https://arxiv.org/pdf/2210.11289.pdf},
doi = {10.48550/arXiv.2210.11289},
eprint={2210.11289},
archivePrefix={arXiv},
primaryClass={cs.LG},
copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
keywords={mine}
}

@unpublished{haddouche2023optimistic,
title = {Optimistic Dynamic Regret Bounds},
author = {Haddouche, Maxime and Guedj, Benjamin and Wintenberger, Olivier},
year = {2023},
note = "Submitted.",
abstract = {Online Learning (OL) algorithms have originally been developed to guarantee good performances when comparing their output to the best fixed strategy. The question of performance with respect to dynamic strategies remains an active research topic. We develop in this work dynamic adaptations of classical OL algorithms based on the use of experts' advice and the notion of optimism. We also propose a constructivist method to generate those advices and eventually provide both theoretical and experimental guarantees for our procedures.},
url = {https://arxiv.org/abs/2301.07530},
url_PDF = {https://arxiv.org/pdf/2301.07530.pdf},
doi = {10.48550/arXiv.2301.07530},
eprint={2301.07530},
archivePrefix={arXiv},
primaryClass={cs.LG},
copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
keywords={mine}
}



